{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\n#import pandas as pd\nos.chdir(\"../input/word2vec-nlp-tutorial\")","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:15.116476Z","start_time":"2021-03-05T02:58:14.979841Z"},"execution":{"iopub.status.busy":"2021-05-21T02:28:36.066955Z","iopub.execute_input":"2021-05-21T02:28:36.067323Z","iopub.status.idle":"2021-05-21T02:28:36.074092Z","shell.execute_reply.started":"2021-05-21T02:28:36.067247Z","shell.execute_reply":"2021-05-21T02:28:36.073190Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!unzip -o labeledTrainData.tsv.zip -d /kaggle/working\n!unzip -o testData.tsv.zip -d /kaggle/working\n!unzip -o unlabeledTrainData.tsv.zip -d /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2021-05-21T02:28:36.078222Z","iopub.execute_input":"2021-05-21T02:28:36.078473Z","iopub.status.idle":"2021-05-21T02:28:39.538796Z","shell.execute_reply.started":"2021-05-21T02:28:36.078449Z","shell.execute_reply":"2021-05-21T02:28:39.537713Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Archive:  labeledTrainData.tsv.zip\n  inflating: /kaggle/working/labeledTrainData.tsv  \nArchive:  testData.tsv.zip\n  inflating: /kaggle/working/testData.tsv  \nArchive:  unlabeledTrainData.tsv.zip\n  inflating: /kaggle/working/unlabeledTrainData.tsv  \n","output_type":"stream"}]},{"cell_type":"code","source":"os.chdir('/kaggle/working')\nwith open('labeledTrainData.tsv','r',encoding='utf-8') as f1:\n    trdata = f1.readlines()\n    trlist = []\n    trrate = []\n    for sent in trdata[1:]:\n        sen = sent.strip('\\n').split('\\t')\n        trlist.append(sen[2].split(' '))\n        trrate.append(sen[1])\n    print(trlist[0])","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:17.232331Z","start_time":"2021-03-05T02:58:16.490498Z"},"execution":{"iopub.status.busy":"2021-05-21T02:28:39.540842Z","iopub.execute_input":"2021-05-21T02:28:39.541231Z","iopub.status.idle":"2021-05-21T02:28:40.226568Z","shell.execute_reply.started":"2021-05-21T02:28:39.541193Z","shell.execute_reply":"2021-05-21T02:28:40.225800Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['\"With', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'MJ', \"i've\", 'started', 'listening', 'to', 'his', 'music,', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there,', 'watched', 'The', 'Wiz', 'and', 'watched', 'Moonwalker', 'again.', 'Maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent.', 'Moonwalker', 'is', 'part', 'biography,', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released.', 'Some', 'of', 'it', 'has', 'subtle', 'messages', 'about', \"MJ's\", 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', \"m'kay.<br\", '/><br', '/>Visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'Michael', 'Jackson', 'so', 'unless', 'you', 'remotely', 'like', 'MJ', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring.', 'Some', 'may', 'call', 'MJ', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'BUT', 'MJ', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him.<br', '/><br', '/>The', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', '20', 'minutes', 'or', 'so', 'excluding', 'the', 'Smooth', 'Criminal', 'sequence', 'and', 'Joe', 'Pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord.', 'Why', 'he', 'wants', 'MJ', 'dead', 'so', 'bad', 'is', 'beyond', 'me.', 'Because', 'MJ', 'overheard', 'his', 'plans?', 'Nah,', 'Joe', \"Pesci's\", 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno,', 'maybe', 'he', 'just', 'hates', \"MJ's\", 'music.<br', '/><br', '/>Lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'MJ', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'Speed', 'Demon', 'sequence.', 'Also,', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'Bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene.<br', '/><br', '/>Bottom', 'line,', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'MJ', 'on', 'one', 'level', 'or', 'another', '(which', 'i', 'think', 'is', 'most', 'people).', 'If', 'not,', 'then', 'stay', 'away.', 'It', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', \"MJ's\", 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl!', 'Michael', 'Jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty?', 'Well,', 'with', 'all', 'the', 'attention', \"i've\", 'gave', 'this', 'subject....hmmm', 'well', 'i', \"don't\", 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors,', 'i', 'know', 'this', 'for', 'a', 'fact.', 'He', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars.', 'I', 'hope', 'he', 'is', 'not', 'the', 'latter.\"']\n","output_type":"stream"}]},{"cell_type":"code","source":"\nwith open('testData.tsv','r',encoding='utf-8') as f1:\n    tedata = f1.readlines()\n    telist = []\n    for sent in tedata[1:]:\n        sen = sent.strip('\\n').split('\\t')\n        telist.append(sen[1].split(' '))\nwith open('unlabeledTrainData.tsv','r',encoding='utf-8') as f1:\n    tr_nolabel = f1.readlines()\n    tr_nolist = []\n    for sent in tr_nolabel[1:]:\n        sen = sent.strip('\\n').split('\\t')\n        tr_nolist.append(sen[1].split(' '))","metadata":{"ExecuteTime":{"end_time":"2021-03-04T14:42:35.086994Z","start_time":"2021-03-04T14:42:31.593511Z"},"execution":{"iopub.status.busy":"2021-05-21T02:28:40.228005Z","iopub.execute_input":"2021-05-21T02:28:40.228346Z","iopub.status.idle":"2021-05-21T02:28:42.616677Z","shell.execute_reply.started":"2021-05-21T02:28:40.228297Z","shell.execute_reply":"2021-05-21T02:28:42.615851Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import argparse\nfrom gensim.models import word2vec\nfrom time import time\n\npath_prefix = './'\nworddata = trlist+telist+tr_nolist\nprint(\"the len of data is {}\".format(len(worddata)))\nst=time()\nmodel = word2vec.Word2Vec(worddata,vector_size=250, window=5, min_count=5, workers=12, epochs=10, sg=0)\nmodel.save(os.path.join(path_prefix, 'w2v_all.model'))\nprint(\"we use time {} min\".format((time()-st)/60))","metadata":{"ExecuteTime":{"end_time":"2021-03-04T15:05:13.806003Z","start_time":"2021-03-04T14:45:54.420801Z"},"execution":{"iopub.status.busy":"2021-05-21T02:28:42.618008Z","iopub.execute_input":"2021-05-21T02:28:42.618384Z","iopub.status.idle":"2021-05-21T02:36:59.992675Z","shell.execute_reply.started":"2021-05-21T02:28:42.618342Z","shell.execute_reply":"2021-05-21T02:36:59.991661Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"the len of data is 100000\nwe use time 8.26547524134318 min\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-05-21T02:36:59.995499Z","iopub.execute_input":"2021-05-21T02:36:59.995846Z","iopub.status.idle":"2021-05-21T02:37:00.004859Z","shell.execute_reply.started":"2021-05-21T02:36:59.995814Z","shell.execute_reply":"2021-05-21T02:37:00.003858Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<gensim.models.word2vec.Word2Vec at 0x7ff6a56855d0>"},"metadata":{}}]},{"cell_type":"code","source":"word2idx = model.wv.key_to_index\nidx2word = model.wv.index_to_key\nembeddingmatrix = []\nfor i,v in enumerate(idx2word):\n    embeddingmatrix.append(model.wv[v])\n#\"PAD\" vector\nvec1 = np.random.rand(250)\n#\"UNK\" vector\nvec2 = np.random.rand(250)\nword2idx[\"PAD\"] = len(word2idx)\nidx2word.append(\"PAD\")\nembeddingmatrix.append(vec1)\nword2idx[\"UNK\"] = len(word2idx)\nidx2word.append(\"UNK\")\nembeddingmatrix.append(vec2)\n#embeddingmatrix = torch.cat([embeddingmatrix,vec1,vec2],0)","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:30.57319Z","start_time":"2021-03-05T02:58:29.971482Z"},"execution":{"iopub.status.busy":"2021-05-21T02:37:00.006750Z","iopub.execute_input":"2021-05-21T02:37:00.007157Z","iopub.status.idle":"2021-05-21T02:37:00.279586Z","shell.execute_reply.started":"2021-05-21T02:37:00.007118Z","shell.execute_reply":"2021-05-21T02:37:00.278816Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# set the length of word is 40\nsenlen = 100\ntrain_num = []\nfor senten in trlist:\n    wordlist = []\n    if len(senten) >= senlen:\n        for sen in senten[:senlen]:\n            if sen in word2idx.keys():\n                wordlist.append(word2idx[sen])\n            else:\n                wordlist.append(word2idx['UNK'])\n        train_num.append(wordlist)\n    else:\n        less_num = senlen - len(senten)\n        for sen in senten:\n            if sen in word2idx.keys():\n                wordlist.append(word2idx[sen])\n            else:\n                wordlist.append(word2idx['UNK'])\n        for _ in range(less_num):\n            wordlist.append(word2idx['PAD'])\n        train_num.append(wordlist)","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:33.14566Z","start_time":"2021-03-05T02:58:32.524247Z"},"execution":{"iopub.status.busy":"2021-05-21T02:37:00.281086Z","iopub.execute_input":"2021-05-21T02:37:00.281438Z","iopub.status.idle":"2021-05-21T02:37:01.400396Z","shell.execute_reply.started":"2021-05-21T02:37:00.281402Z","shell.execute_reply":"2021-05-21T02:37:01.399484Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils import data\n# define tordata type 自定义数据集类\nclass moviedata(data.Dataset):\n    def __init__(self, X, y):\n        self.data = X\n        self.label = y\n    def __getitem__(self, idx):\n        if self.label is None: return self.data[idx]\n        return self.data[idx], self.label[idx]\n    def __len__(self):\n        return len(self.data)","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:35.581961Z","start_time":"2021-03-05T02:58:35.038075Z"},"execution":{"iopub.status.busy":"2021-05-21T02:37:01.401795Z","iopub.execute_input":"2021-05-21T02:37:01.402181Z","iopub.status.idle":"2021-05-21T02:37:02.956882Z","shell.execute_reply.started":"2021-05-21T02:37:01.402144Z","shell.execute_reply":"2021-05-21T02:37:02.956080Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom torch.autograd import Variable\nclass gru_net(nn.Module):\n    def __init__(self,embedding, hidden_dim, num_layers, dropout=0.5,embedding_train=False):\n        super(gru_net, self).__init__()\n#         embedding = torch.Tensor(len(list(word_vectors)), len(word_vectors[0]))\n#         word2vector_list = list(word_vectors)\n#         for i in range(len(word2vector_list)-1):\n#             get_vectors = word2vector_list[i]\n#             embedding[i] = get_vectors\n#         self.embedding = nn.Embedding.from_pretrained(embedding)\n        self.embedding = torch.nn.Embedding(num_embeddings=len(embedding), embedding_dim=len(embedding[0]))\n        self.embedding.weight.data.copy_(torch.Tensor(embedding.float()))\n        self.embedding.weight.requires_grad = False\n        \n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.gru = nn.GRU(len(embedding[0]),self.hidden_dim,num_layers=self.num_layers,batch_first=True)\n        \n        self.classfier = nn.Sequential(\n            nn.Dropout(self.dropout),\n            nn.Linear(self.hidden_dim,1),\n            nn.Sigmoid()\n        )\n    def forward(self,inputs):\n        inputs = self.embedding(inputs.cuda())\n        inputs = inputs.float().cuda()\n        inputs = Variable(inputs,requires_grad = True)\n        x, _ = self.gru(inputs, None)\n        x = x[:,-1,:]# x is [batch,seq_len,hidden_size]\n        x = self.classfier(x)\n        return x","metadata":{"ExecuteTime":{"end_time":"2021-03-05T03:10:09.521453Z","start_time":"2021-03-05T03:10:09.489445Z"},"execution":{"iopub.status.busy":"2021-05-21T02:37:02.958114Z","iopub.execute_input":"2021-05-21T02:37:02.958469Z","iopub.status.idle":"2021-05-21T02:37:02.971109Z","shell.execute_reply.started":"2021-05-21T02:37:02.958433Z","shell.execute_reply":"2021-05-21T02:37:02.970221Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def evaluation(outputs, labels):\n    # outputs => probability (float)\n    # labels => labels\n    outputs[outputs>=0.5] = 1 # 大於等於 0.5 為有惡意\n    outputs[outputs<0.5] = 0 # 小於 0.5 為無惡意\n    correct = torch.sum(torch.eq(outputs, labels)).item()\n    return correct","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:45.133909Z","start_time":"2021-03-05T02:58:45.126901Z"},"execution":{"iopub.status.busy":"2021-05-21T02:37:02.972503Z","iopub.execute_input":"2021-05-21T02:37:02.972842Z","iopub.status.idle":"2021-05-21T02:37:02.984004Z","shell.execute_reply.started":"2021-05-21T02:37:02.972807Z","shell.execute_reply":"2021-05-21T02:37:02.983209Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nimport argparse\nimport numpy as np\nfrom torch import nn\nfrom gensim.models import word2vec\nimport torch.optim as optim\n#from sklearn.model_selection import train_test_split\n","metadata":{"ExecuteTime":{"end_time":"2021-03-05T02:58:50.185803Z","start_time":"2021-03-05T02:58:50.180786Z"},"execution":{"iopub.status.busy":"2021-05-21T02:37:02.985416Z","iopub.execute_input":"2021-05-21T02:37:02.985996Z","iopub.status.idle":"2021-05-21T02:37:02.994325Z","shell.execute_reply.started":"2021-05-21T02:37:02.985955Z","shell.execute_reply":"2021-05-21T02:37:02.993532Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nbest_valid_loss = float('inf') # 保存最小的loss\nprint(best_valid_loss)\ndef train_process(batch_size,epoch,lr,model_dir,train_loader,val_loader,gmodel,device,criterion):\n    global best_valid_loss\n    for i in range(epoch):\n        train_loss = 0\n        train_correct = 0\n        val_loss = 0\n        val_correct = 0\n        for batch in train_loader:\n            optimizer = optim.Adam(gmodel.parameters(), lr)\n            optimizer.zero_grad()\n            X, labels = batch\n            labels.cuda()\n            pred = gmodel(X).squeeze(1)\n            loss = criterion(pred, labels.float().cuda())\n            loss.backward()\n            optimizer.step()# 更新权重\n            train_loss += loss.item()\n            train_correct += evaluation(pred, labels.cuda())\n        with torch.no_grad():\n            for batch in val_loader:\n                X, labels = batch\n                pred = gmodel(X).squeeze(1)\n                loss = criterion(pred, labels.float().cuda())\n                val_loss += loss.item()\n                val_correct += evaluation(pred,labels.cuda())\n            if val_loss < best_valid_loss :\n                best_valid_loss = val_loss\n                torch.save(gmodel.state_dict(), './gmodel.pt')\n                \n        print(\"epoch:\",i,\"train_loss:\",train_loss,\"train_correct:\",train_correct)\n        print(\"epoch:\",i,\"val_loss:\",val_loss,\"val_correct:\",val_correct)","metadata":{"execution":{"iopub.status.busy":"2021-05-21T02:54:10.730292Z","iopub.execute_input":"2021-05-21T02:54:10.730647Z","iopub.status.idle":"2021-05-21T02:54:10.743650Z","shell.execute_reply.started":"2021-05-21T02:54:10.730613Z","shell.execute_reply":"2021-05-21T02:54:10.741431Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"inf\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nembedding_train=False # fix embedding during training\nbatch_size = 256\nepoch = 100\nlr = 0.002\nmodel_dir = './'\ncriterion = nn.BCEWithLogitsLoss() \ncriterion = criterion.to(device)\nprint(len(train_num))\nX = torch.LongTensor(train_num)\ny = [int(rate) for rate in trrate]\ny = torch.LongTensor(y)\nembeddingmatrix = torch.from_numpy(np.array(embeddingmatrix))\n#embeddingmatrix = torch.tensor(embeddingmatrix)\n#establish model\ngmodel = gru_net(embeddingmatrix, hidden_dim=150, num_layers=1, dropout=0.5,embedding_train=embedding_train)\ngmodel = gmodel.to(device)\n# split data\nx_train,x_valid,y_train,y_valid = X[:18000],X[18000:],y[:18000],y[18000:]\n# dataloader\ntrain_dataset = moviedata(X=x_train, y=y_train)\nval_dataset = moviedata(X=x_valid, y=y_valid)\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                            batch_size = batch_size,\n                                            shuffle = True,\n                                            num_workers = 4)\n\nval_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n                                            batch_size = batch_size,\n                                            shuffle = False,\n                                            num_workers = 4)\ntorch.cuda.empty_cache()\ntrain_process(batch_size, epoch, lr, model_dir, train_loader, val_loader, gmodel, device,criterion)\n\ntest_data = []\nsenlen = 100\nfor sen in telist:\n    wordlist=[]\n    if len(sen) >= senlen:\n        for word in sen[:senlen]:\n            if word in word2idx.keys():\n                wordlist.append(word2idx[word])\n            else:\n                wordlist.append(word2idx['UNK'])\n        test_data.append(wordlist)\n    else:\n        for word in sen[:len(sen)]:\n            if word in word2idx.keys():\n                wordlist.append(word2idx[word])\n            else:\n                wordlist.append(word2idx['UNK'])\n        for i in range(senlen - len(sen)) :\n            wordlist.append(word2idx['PAD'])\n        test_data.append(wordlist)\ntest_data = torch.LongTensor(test_data)\ngmodel.eval()\ndf = pd.read_csv('./testData.tsv',quoting = 3,delimiter='\\t')\nsample = df[['id']]\nresult = []\nwith torch.no_grad():\n    for X in test_data:\n        pred = gmodel(X.unsqueeze(0))\n        if pred >= 0.5:\n            pred = 1\n        else:\n            pred = 0\n        result.append(pred)\nsample['sentiment'] = result\nsample.to_csv('gru_model.csv',\n             index=False, quoting=3)\n        \n    ","metadata":{"ExecuteTime":{"end_time":"2021-03-05T03:12:18.703502Z","start_time":"2021-03-05T03:12:17.741486Z"},"execution":{"iopub.status.busy":"2021-05-21T04:18:04.327061Z","iopub.execute_input":"2021-05-21T04:18:04.327409Z","iopub.status.idle":"2021-05-21T04:22:28.982664Z","shell.execute_reply.started":"2021-05-21T04:18:04.327379Z","shell.execute_reply":"2021-05-21T04:22:28.981814Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"25000\nepoch: 0 train_loss: 47.97110956907272 train_correct: 10469\nepoch: 0 val_loss: 17.548724949359894 val_correct: 4920\nepoch: 1 train_loss: 44.86416482925415 train_correct: 13182\nepoch: 1 val_loss: 18.33252340555191 val_correct: 5144\nepoch: 2 train_loss: 43.55146384239197 train_correct: 13847\nepoch: 2 val_loss: 16.95937007665634 val_correct: 5434\nepoch: 3 train_loss: 43.162538290023804 train_correct: 14018\nepoch: 3 val_loss: 17.150950968265533 val_correct: 5076\nepoch: 4 train_loss: 42.72477573156357 train_correct: 14078\nepoch: 4 val_loss: 16.773990869522095 val_correct: 5596\nepoch: 5 train_loss: 42.335575580596924 train_correct: 14397\nepoch: 5 val_loss: 16.595870554447174 val_correct: 5646\nepoch: 6 train_loss: 41.888813614845276 train_correct: 14665\nepoch: 6 val_loss: 16.76060599088669 val_correct: 5649\nepoch: 7 train_loss: 41.71377170085907 train_correct: 14708\nepoch: 7 val_loss: 16.892637610435486 val_correct: 5254\nepoch: 8 train_loss: 41.395682752132416 train_correct: 14936\nepoch: 8 val_loss: 16.533733367919922 val_correct: 5579\nepoch: 9 train_loss: 41.0260129570961 train_correct: 15147\nepoch: 9 val_loss: 16.666413009166718 val_correct: 5679\nepoch: 10 train_loss: 40.79119807481766 train_correct: 15312\nepoch: 10 val_loss: 16.47638690471649 val_correct: 5663\nepoch: 11 train_loss: 40.60615289211273 train_correct: 15396\nepoch: 11 val_loss: 16.7410826086998 val_correct: 5680\nepoch: 12 train_loss: 40.31609946489334 train_correct: 15547\nepoch: 12 val_loss: 16.473099172115326 val_correct: 5752\nepoch: 13 train_loss: 40.21214282512665 train_correct: 15580\nepoch: 13 val_loss: 17.067935287952423 val_correct: 5628\nepoch: 14 train_loss: 39.98379874229431 train_correct: 15697\nepoch: 14 val_loss: 16.45883148908615 val_correct: 5726\nepoch: 15 train_loss: 39.74194127321243 train_correct: 15896\nepoch: 15 val_loss: 16.41105091571808 val_correct: 5675\nepoch: 16 train_loss: 39.620628118515015 train_correct: 15918\nepoch: 16 val_loss: 16.371284067630768 val_correct: 5757\nepoch: 17 train_loss: 39.53128832578659 train_correct: 15969\nepoch: 17 val_loss: 16.41828715801239 val_correct: 5725\nepoch: 18 train_loss: 39.43071335554123 train_correct: 16024\nepoch: 18 val_loss: 16.54533565044403 val_correct: 5749\nepoch: 19 train_loss: 39.346126317977905 train_correct: 16081\nepoch: 19 val_loss: 16.548381447792053 val_correct: 5755\nepoch: 20 train_loss: 39.23003673553467 train_correct: 16173\nepoch: 20 val_loss: 16.502315938472748 val_correct: 5698\nepoch: 21 train_loss: 39.11978793144226 train_correct: 16197\nepoch: 21 val_loss: 16.714393377304077 val_correct: 5735\nepoch: 22 train_loss: 39.05582481622696 train_correct: 16223\nepoch: 22 val_loss: 16.478145599365234 val_correct: 5594\nepoch: 23 train_loss: 38.91459542512894 train_correct: 16314\nepoch: 23 val_loss: 16.53898310661316 val_correct: 5757\nepoch: 24 train_loss: 38.79106563329697 train_correct: 16326\nepoch: 24 val_loss: 16.678233742713928 val_correct: 5747\nepoch: 25 train_loss: 38.72435784339905 train_correct: 16428\nepoch: 25 val_loss: 16.42967587709427 val_correct: 5717\nepoch: 26 train_loss: 38.6451535820961 train_correct: 16440\nepoch: 26 val_loss: 16.446450114250183 val_correct: 5709\nepoch: 27 train_loss: 38.676606833934784 train_correct: 16457\nepoch: 27 val_loss: 16.678032219409943 val_correct: 5739\nepoch: 28 train_loss: 38.602405071258545 train_correct: 16509\nepoch: 28 val_loss: 16.606262624263763 val_correct: 5730\nepoch: 29 train_loss: 38.57588291168213 train_correct: 16521\nepoch: 29 val_loss: 16.43356281518936 val_correct: 5698\nepoch: 30 train_loss: 38.617676734924316 train_correct: 16486\nepoch: 30 val_loss: 16.519730269908905 val_correct: 5750\nepoch: 31 train_loss: 38.46758681535721 train_correct: 16562\nepoch: 31 val_loss: 16.537228643894196 val_correct: 5763\nepoch: 32 train_loss: 38.46268731355667 train_correct: 16581\nepoch: 32 val_loss: 16.511479437351227 val_correct: 5784\nepoch: 33 train_loss: 38.49654060602188 train_correct: 16558\nepoch: 33 val_loss: 16.468404352664948 val_correct: 5749\nepoch: 34 train_loss: 38.37054592370987 train_correct: 16626\nepoch: 34 val_loss: 16.53522425889969 val_correct: 5766\nepoch: 35 train_loss: 38.36251509189606 train_correct: 16600\nepoch: 35 val_loss: 16.392772376537323 val_correct: 5781\nepoch: 36 train_loss: 38.35335636138916 train_correct: 16625\nepoch: 36 val_loss: 16.38202303647995 val_correct: 5756\nepoch: 37 train_loss: 38.24818575382233 train_correct: 16670\nepoch: 37 val_loss: 16.440190613269806 val_correct: 5790\nepoch: 38 train_loss: 38.24651098251343 train_correct: 16691\nepoch: 38 val_loss: 16.443048059940338 val_correct: 5739\nepoch: 39 train_loss: 38.21763116121292 train_correct: 16693\nepoch: 39 val_loss: 16.665552854537964 val_correct: 5749\nepoch: 40 train_loss: 38.25708955526352 train_correct: 16685\nepoch: 40 val_loss: 16.40772271156311 val_correct: 5721\nepoch: 41 train_loss: 38.196314454078674 train_correct: 16706\nepoch: 41 val_loss: 16.399895548820496 val_correct: 5753\nepoch: 42 train_loss: 38.2120206952095 train_correct: 16717\nepoch: 42 val_loss: 16.475682377815247 val_correct: 5738\nepoch: 43 train_loss: 38.12419128417969 train_correct: 16752\nepoch: 43 val_loss: 16.43016415834427 val_correct: 5625\nepoch: 44 train_loss: 38.15344953536987 train_correct: 16718\nepoch: 44 val_loss: 16.588211238384247 val_correct: 5761\nepoch: 45 train_loss: 38.140664517879486 train_correct: 16722\nepoch: 45 val_loss: 16.50023764371872 val_correct: 5757\nepoch: 46 train_loss: 38.04835295677185 train_correct: 16781\nepoch: 46 val_loss: 16.425040304660797 val_correct: 5732\nepoch: 47 train_loss: 38.082397162914276 train_correct: 16766\nepoch: 47 val_loss: 16.51887422800064 val_correct: 5766\nepoch: 48 train_loss: 38.13700246810913 train_correct: 16741\nepoch: 48 val_loss: 16.386225879192352 val_correct: 5743\nepoch: 49 train_loss: 38.08398303389549 train_correct: 16751\nepoch: 49 val_loss: 16.40302962064743 val_correct: 5742\nepoch: 50 train_loss: 38.1225249171257 train_correct: 16730\nepoch: 50 val_loss: 16.426672339439392 val_correct: 5757\nepoch: 51 train_loss: 38.09203439950943 train_correct: 16762\nepoch: 51 val_loss: 16.415167272090912 val_correct: 5777\nepoch: 52 train_loss: 38.0749252140522 train_correct: 16750\nepoch: 52 val_loss: 16.452094078063965 val_correct: 5734\nepoch: 53 train_loss: 38.08833855390549 train_correct: 16769\nepoch: 53 val_loss: 16.518917560577393 val_correct: 5758\nepoch: 54 train_loss: 38.07336324453354 train_correct: 16778\nepoch: 54 val_loss: 16.566033244132996 val_correct: 5765\nepoch: 55 train_loss: 38.063726127147675 train_correct: 16761\nepoch: 55 val_loss: 16.409734666347504 val_correct: 5725\nepoch: 56 train_loss: 37.96713984012604 train_correct: 16803\nepoch: 56 val_loss: 16.487268149852753 val_correct: 5748\nepoch: 57 train_loss: 37.95796376466751 train_correct: 16821\nepoch: 57 val_loss: 16.64258748292923 val_correct: 5745\nepoch: 58 train_loss: 38.053037881851196 train_correct: 16793\nepoch: 58 val_loss: 16.62794852256775 val_correct: 5730\nepoch: 59 train_loss: 38.03069323301315 train_correct: 16811\nepoch: 59 val_loss: 16.424778759479523 val_correct: 5726\nepoch: 60 train_loss: 38.09250223636627 train_correct: 16772\nepoch: 60 val_loss: 16.643457651138306 val_correct: 5743\nepoch: 61 train_loss: 38.02481549978256 train_correct: 16800\nepoch: 61 val_loss: 16.477692663669586 val_correct: 5761\nepoch: 62 train_loss: 38.009071946144104 train_correct: 16790\nepoch: 62 val_loss: 16.43182349205017 val_correct: 5739\nepoch: 63 train_loss: 37.95571148395538 train_correct: 16827\nepoch: 63 val_loss: 16.63387554883957 val_correct: 5731\nepoch: 64 train_loss: 38.079925775527954 train_correct: 16760\nepoch: 64 val_loss: 16.42437618970871 val_correct: 5704\nepoch: 65 train_loss: 37.988595485687256 train_correct: 16811\nepoch: 65 val_loss: 16.5952450633049 val_correct: 5751\nepoch: 66 train_loss: 38.00382858514786 train_correct: 16824\nepoch: 66 val_loss: 16.45209050178528 val_correct: 5743\nepoch: 67 train_loss: 37.93483501672745 train_correct: 16845\nepoch: 67 val_loss: 16.488009870052338 val_correct: 5769\nepoch: 68 train_loss: 37.938767433166504 train_correct: 16850\nepoch: 68 val_loss: 16.459339559078217 val_correct: 5752\nepoch: 69 train_loss: 37.978011429309845 train_correct: 16825\nepoch: 69 val_loss: 16.51132208108902 val_correct: 5732\nepoch: 70 train_loss: 38.061999171972275 train_correct: 16761\nepoch: 70 val_loss: 16.501590967178345 val_correct: 5688\nepoch: 71 train_loss: 37.998686373233795 train_correct: 16790\nepoch: 71 val_loss: 16.577210426330566 val_correct: 5737\nepoch: 72 train_loss: 37.9892840385437 train_correct: 16815\nepoch: 72 val_loss: 16.484640061855316 val_correct: 5710\nepoch: 73 train_loss: 37.942569732666016 train_correct: 16841\nepoch: 73 val_loss: 16.467912316322327 val_correct: 5669\nepoch: 74 train_loss: 37.986052095890045 train_correct: 16808\nepoch: 74 val_loss: 16.541998147964478 val_correct: 5766\nepoch: 75 train_loss: 38.0284266769886 train_correct: 16800\nepoch: 75 val_loss: 16.617140293121338 val_correct: 5744\nepoch: 76 train_loss: 37.94718337059021 train_correct: 16820\nepoch: 76 val_loss: 16.445965826511383 val_correct: 5716\nepoch: 77 train_loss: 38.02287793159485 train_correct: 16795\nepoch: 77 val_loss: 16.666426360607147 val_correct: 5722\nepoch: 78 train_loss: 37.99368089437485 train_correct: 16811\nepoch: 78 val_loss: 16.59363543987274 val_correct: 5738\nepoch: 79 train_loss: 38.03517121076584 train_correct: 16795\nepoch: 79 val_loss: 16.555773556232452 val_correct: 5668\nepoch: 80 train_loss: 37.995763659477234 train_correct: 16769\nepoch: 80 val_loss: 16.50915688276291 val_correct: 5747\nepoch: 81 train_loss: 38.04653990268707 train_correct: 16768\nepoch: 81 val_loss: 16.492547869682312 val_correct: 5760\nepoch: 82 train_loss: 38.01892274618149 train_correct: 16782\nepoch: 82 val_loss: 16.414264976978302 val_correct: 5731\nepoch: 83 train_loss: 38.03670597076416 train_correct: 16769\nepoch: 83 val_loss: 16.48118382692337 val_correct: 5741\nepoch: 84 train_loss: 38.049263179302216 train_correct: 16767\nepoch: 84 val_loss: 16.467012465000153 val_correct: 5750\nepoch: 85 train_loss: 38.00549751520157 train_correct: 16812\nepoch: 85 val_loss: 16.38541978597641 val_correct: 5775\nepoch: 86 train_loss: 38.01898056268692 train_correct: 16803\nepoch: 86 val_loss: 16.39042103290558 val_correct: 5760\nepoch: 87 train_loss: 38.00514632463455 train_correct: 16805\nepoch: 87 val_loss: 16.486055374145508 val_correct: 5761\nepoch: 88 train_loss: 38.062576323747635 train_correct: 16786\nepoch: 88 val_loss: 16.39156264066696 val_correct: 5735\nepoch: 89 train_loss: 38.137519389390945 train_correct: 16786\nepoch: 89 val_loss: 16.470592200756073 val_correct: 5765\nepoch: 90 train_loss: 37.88589906692505 train_correct: 16870\nepoch: 90 val_loss: 16.622195065021515 val_correct: 5753\nepoch: 91 train_loss: 37.944229662418365 train_correct: 16834\nepoch: 91 val_loss: 16.452118635177612 val_correct: 5731\nepoch: 92 train_loss: 37.929906606674194 train_correct: 16857\nepoch: 92 val_loss: 16.421954572200775 val_correct: 5739\nepoch: 93 train_loss: 38.00494796037674 train_correct: 16835\nepoch: 93 val_loss: 16.588616847991943 val_correct: 5746\nepoch: 94 train_loss: 37.98407047986984 train_correct: 16844\nepoch: 94 val_loss: 16.469956517219543 val_correct: 5758\nepoch: 95 train_loss: 37.9563205242157 train_correct: 16836\nepoch: 95 val_loss: 16.434771955013275 val_correct: 5710\nepoch: 96 train_loss: 37.94133299589157 train_correct: 16845\nepoch: 96 val_loss: 16.543883562088013 val_correct: 5752\nepoch: 97 train_loss: 37.87619996070862 train_correct: 16860\nepoch: 97 val_loss: 16.42834174633026 val_correct: 5775\nepoch: 98 train_loss: 38.01129972934723 train_correct: 16787\nepoch: 98 val_loss: 16.54653376340866 val_correct: 5759\nepoch: 99 train_loss: 37.959953397512436 train_correct: 16825\nepoch: 99 val_loss: 16.47314417362213 val_correct: 5725\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}